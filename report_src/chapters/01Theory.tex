\documentclass[../main.tex]{subfiles}
\begin{document}
\subsection{What is the core idea of deep learning? How does it differ from shallow learning?}

The core idea of deep learning is to have several hidden layers to describe complicated functions. This enables more complex representations, with the disadvantage of more computing power needed, as well as overfitting. \autoref{fig:deep_example} shows a diagram of a deep neural network, where $\mathbf{u}=\{u_1, u_2\}$, $\mathbf{v}=\{v_1, v_2\}$, and the cloud denotes hidden layers. 

A shallow network contains fewer, if any, hidden layers. It can represent all functions, but the number of parameters expands rapidly with the complexity of the function. It is less prone to overfitting. A shallow network is displayed in \autoref{fig:shallow_example}, where $\mathbf{u}=\{u_1, u_2\}$ is the only hidden layer.

\begin{figure}
    \centering
    \begin{subfigure}[b]{0.48\textwidth}
    	\centering
        \includegraphics[height=3cm]{figures/theory/deep_network}
        \caption{Example of a deep network}
        \label{fig:deep_example}
    \end{subfigure}
    ~
    \begin{subfigure}[b]{0.48\textwidth}
    	\centering
        \includegraphics[height=3cm]{figures/theory/shallow_network}
        \caption{Example of a shallow network}
        \label{fig:shallow_example}
    \end{subfigure}
    \caption{Deep and shallow networks.}\label{fig:deep_shallow}
\end{figure}


\subsection{Logistic regression as neural network}

One can define the logistic regression implemented in Assignment 1 \cite{assignment1} as a neural network, where we have one hidden layer with one neuron. One first define the input to the neuron as
\begin{equation}
	z = h(\mbf{x}) =\mbf{w}^{\rm T} \mbf{x}
\end{equation}
The activation function is then defined as
\begin{equation}
	\sigma(z)=\frac{1}{1+e^{-z}}
\end{equation}
and the backpropagation algorithm is then
\begin{equation}
	\frac{\partial E_{\rm ec}(\mathbf{w})}{\partial \mathbf{w}} = 
		\frac{\partial E_{\rm ec}(\mathbf{w})}{\partial \sigma(z)}
		\cdot \frac{\partial \sigma(z)}{\partial z}
		\cdot \frac{\partial z}{\partial \mathbf{w}}
\end{equation}
The training will then be
\begin{equation}
	\mbf{w}(k+1) \leftarrow \mbf{w}(k) - \eta \sum_{i=1}^{N}(\sigma(z)-y_i)\mbf{x}_i
\end{equation}
The topology of the network is shown in \autoref{fig:log_example}.

\begin{figure}
	\centering
    \includegraphics[height=3cm]{figures/theory/log_network}
    \caption{Example of a logistic regression neural network}
    \label{fig:log_example}
\end{figure}


\end{document}